System organization.

Describes basic components comprising computer systems (CPU, memory, I/O, buses). One can write software ignorant of these concepts, but high performance software requires a complete understanding of them. 

Basic system components.
The basic operational design of a computer system is called its architecture. John von Neumann is given credit for the architecture of most computers. A typical von Neumann architecture (VNA) has three components: the CPU, memory, and I/O. The CPU is where all computations occur in VNAs. Data and CPU instructions reside in memory until the CPU requires them. Most I/O devices look like memory to the CPU because CPU can store data to an output device and read data from an input device. The difference between memory and I/O is that I/O locations associated with external devices.

The system bus.
The system bus connects the components of VNAs. The 80x86 family has three chief buses: address, data, and control. Though buses vary from processor to processor, each bus carries comparable information on all processors. A typical 80x86 system component uses standard TTL (transistor-transistor logic) logic levels, meaning that each wire on a bus uses a standard voltage level to represent zero and one. 

The data bus.
The 80x86 processors use the data bus to shuffle data between various components in a computer system. This bus defines the "size" of the processor. The size of a processor is generally the number of data lines on that processor. Having a n-bit data bus does not limit the processor to n-bit data types; it means that the processor can access n bits of data per memory cycle. 

The address bus.
We know that the data bus transfers information between a particular memory location or I/O device and the CPU. What memory location or I/O device, though? The answer is in the address bus. The system designer assigns a unique memory address to each memory element and I/O device. When software wants to access some memory in one of those locations, it places the corresponding address on the address bus. Memory or I/O circuitry recognizes the address and instructs the memory or I/O device to read the data from or place data on the data bus. A processor can create two unique addresses with a single address line and 2**n addresses with n address lines. The number of bits on an address bus therefore determines the maximum number of addressable memory and I/O locations.

The control bus. 
The control bus is an eclectic collection of signals that controls how the processor communicates with the rest of the system. If we consider the data bus, the CPU sends data to memory and receives data from memory on the data bus. Is it sending or receiving, though? The control bus has two lines, read and write, that specify the data flow direction. Signals may also include system blocks, interrupt lines, and status lines. When the read and write control lines both contain a logic one, the CPU and memory-I/O are not communicating. If the read line is low (logic zero), CPU is reading data from memory. If the write line is low, data is being transferred from the CPU to memory. Byte enable lines allow processors to deal with smaller chunks of data. The 80x86 family provides distinct address spaces for memory and I/O, though it does not have two address buses. Additional control lines decide whether an address is intended for memory or I/O. When these signals are active, I/O devices use the address on the LO 16 bits of the address bus, and ignore the signals on the address bus when these signals are inactive (when the memory subsystem takes over).

The memory subsystem.
What is a memory location, exactly? The 80x86 supports byte-addressable memory, so the basic memory unit is a byte. Think of memory as a linear array of bytes, where the address of the first byte is zero and the address of the last byte is 2**n - 1. To execute the equivalent of `Memory [125] := 0;` in Pascal, the CPU places the value 0 on the data bus, the address 125 on the address bus, and asserts the write line. To execute the equivalent of `CPU := Memory [125];` in Pascal, the CPU places the address 125 on the address bus, asserts the read line, and reads the resulting data from the data bus. What happens when the processor needs to access something larger than a byte? The 80x86 family deals with this problem by storing the LO byte of a word at the address specified and the HO byte at the next location. For double words, the remaining two bytes are stores in the next two memory locations. Note the possibility of byte, word, and double word values overlapping in memory. For example, we could have a word variable at address 193, a byte variable at address 194, and a double word variable at address 192. "Byte-addressable memory array" means that the CPU can address memory in chunks as small as one byte (no smaller). Note that addresses are integers. Loading a word requires two memory operations and a double word requires four memory operations. Sixteen-bit processors organize memory into two banks: an even and odd bank. They can load a word from any arbitrary address. Look at figure 3.7, though. If we want to read a word from address 125, we take the data from addresses 125 and 126. There are two problems here. Data bus lines eight through fifteen (the HO byte) connect to the odd bank and the LO byte connects to the even bank. This data transfer would result in the two bytes being swapped. The CPUs on the 80x86 accommodate for this problem. Secondly, when accessing a word, we are really accessing two separate bytes at two addresses. What address appears on the address bus? Sixteen-bit 80x86 CPUs place even addresses on the bus. Even bytes are on the LO byte and odd bytes are on the HO byte. If you access a word at an even address, the CPU can bring the sixteen-bit chunk in one memory operation. If you access a single byte, the CPU activates the appropriate bank (using a "byte-enable" control line). If the byte appeared at an odd address, the CPU will automatically move it from the HO byte on the bus to the LO byte. How about the CPU accessing a word at an odd address? The CPU cannot place an odd address onto the address bus and read the sixteen bits from memory. Addresses are always even; so, if you try to put (for instance) 125 on the address bus, 124 will be put on the address bus and the word formed by concatenating the bytes at 124 and 125 would result. Two memory operations are required to access a word at an odd address. The CPU must read the byte address at 125 and then read the byte address at 126. It will then swap the positions of these bytes internally. Even though the processor hides these details from you so it is simple to access odd-addressed words, carefully arranging how you use memory so that you avoid storing word or double word values at odd addresses will improve the speed of the program. Thirty-two bit quantity storage requires at least two memory operations. It will require three memory operations if stored at an odd address. Thirty-two bit processors use four banks of memory connected to the thirty-two–bit data bus. The address placed on the address bus is some multiple of four. Using byte-enable lines, the CPU selects which of the four bytes at that address the software wants to access. CPU will automatically rearrange bytes as necessary in sixteen-bit processors. With a thirty-two–bit memory interface, the CPU can access any byte with one memory operation. If the address modulus four does not equal three, a thirty-two–bit CPU can access a word at that address with one memory operation. If it does, it will take two memory operations (see figure 3.9). The thirty-two bit CPU can access a double word in one memory operation if the address is divisible by four. It will require two operations otherwise. To reiterate, the CPU handles all correct data loading. There is a massive performance benefit to align your data addressing according to what has been stated. As a rule, place word values at even addresses and double word values at addresses divisible by four. 

The I/O subsystem.
The 80x86 family provides a sixteen-bit I/O address bus. As previously mentioned, this gives the CPU two separate address spaces: one for memory and one for I/O. Other than separate control lines and a smaller bus, I/O addressing behaves exactly like memory addressing. 

System timing.
von Neumann machines are serialized, meaning that the computer executes commands in a prescribed order. How does the processor execute statements and how do we measure how long they take to execute? The CPU is a very complex piece of circuitry. Without going into too many details, let us just say that operations inside the CPU must be very carefully coordinated or the CPU will produce erroneous results. To ensure that all operations occur at just the right moment, the 80x86 CPUs use an alternating signal called the system clock.

The system clock.
The system clock handles all synchronization within a computer system. It is an electrical signal on the control bus alternating between zero and one periodically. System clock gates many of the logic gates that make up the CPU, allowing them to operate in a synchronized fashion. The system clock frequency is the frequency at which the system clock alternates between zero and one. The clock period or clock cycle refers to one period of the alternation. Of course, one clock period is the reciprocal of the clock frequency. Most CPUs start an operation on either the falling edge or the rising edge. Since the clock spends negligible time switching from zero to one or vice versa, the clock edge is a perfect synchronization point. Note that many operations take longer than one clock cycle, so the operation rate of the CPU is realistically lower than the clock frequency (mostly).

Memory access and the system clock.
Memory access is the most common CPU activity and synchronized around the system clock. The memory access time is the number of clock cycles the system requires to access a memory location. It is an important value because longer memory access times result in reduced performance. Some 80x86 processors require four clock cycles while others require one. Memory access time is the amount of time between a memory operation request and the time it completes. When reading from memory, the memory access time is the amount of time from the point that the CPU places an address on the address bus and the CPU takes the data off the data bus. Two standard memory device ratings are capacity and speed (access time). Typical dynamic RAM devices have capacities of four or more MB and speeds of fifty to one hundred nanoseconds. 

Wait states.
A wait state is an extra clock cycle to give some device time to complete an operation. For example, a 50 MHz 80486 system has a 20 ns clock period which implies that you need 20 ns memory. Most computer systems have additional circuitry between CPU and memory (decoding and buffering logic) that introduce additional delays into the system. Figure 3.13 shows a system losing 10 ns to buffering and decoding. If the CPU needs the data back in 20 ns, the memory must respond in less than 10 ns. You can buy 10 ns memory, but it is expensive, bulky, and produces a lot of heat. Only machines like supercomputers use this type of memory. Wait states partly solve this problem without the drawbacks of those supercomputer aspects. If you have a 20 MHz processor with memory cycle time 50 ns and you lose 10 ns due to buffering and decoding, you need 40 ns memory. What if 80 ns memory is all you can afford? Adding a wait state to extend the memory cycle to 100 ns (two clock cycles) will solve this problem. Subtracting 10 ns for the decoding and buffering leavs 90 ns, and therefore 80 ns memory will respond well before the CPU requires the data. Almost all CPUs provide a signal on the control bus to allow wait state insertion. The decoding circuitry generally asserts this line to delay one additional clock period if necessary, which gives the memory sufficient access time. Consider the 50 MHz 80486 whose cycle time is less than 20 ns. Less than 10 ns are available after decoding and buffering. One wait state in a 60 ns memory system will not do the trick, since each wait state gives you 20 ns. Three wait states would have to do. Wait states are not a good thing from a performance point of view since while the CPU is waiting for data from memory it cannot operate on it. Adding one wait state to a memory cycle on an 80486 CPU doubles the time required to access the data and thus halves the sleep of the memory access. Though systems claim to run at zero wait state RAM, most are lying. They figure that if their system ever operates at zero wait states, they can make that claim in their literature. There are several tricks hardware designers use to achieve zero wait states most of the time, the most common of which is using cache memory.

Cache memory.
A typical program tends to access the same memory location repeatedly. It often accesses adjacent memory locations. The names given to these phenomena are temporal locality of reference and spatial locality of reference. When exhibiting spatial locality, a program accesses neighbouring memory locations. When displaying temporal locality of reference a program repeatedly accesses the same memory location during a short time period. There are two occurrences of each locality form in this Pascal code block: 

for i := 0 to 10 do
	A [i] := 0;

The program references i several times. The loop compares i to 10 to see if the loop is complete, it increments i by one at the bottom of the loop, and it uses i as an array index. These things demonstrate temporal locality of reference. The loop zeros out the elements of A by writing a zero to the first location in A, then to the second, and so on. Assuming that Pascal stores the elements of A into consecutive memory locations, each loop iteration accesses adjacent memory locations. Computer instructions, which tell the system to do a specified task, are also in memory. They appear sequentially and the system executes them repeatedly. This fact demonstrates another aspect of spatial and temporal locality. Upon inspection of a typical program's execution profile, you would discover that the program typically executes less than half the statements. It may use ten to twenty percent of the memory allotted to it. At any one given time, a one megabyte pro- gram might only access four to eight kilobytes of data and code. Wouldn't it be nice to buy a small amount of fast RAM and dynamically reassign its addresses as the program executes? Cache memory does just that. This small amount of very fast (zero wait state) memory sits between the CPU and main memory. The bytes appearing within a cache do not have fixed addresses; instead, it can reassign the address of a data object, allowing the system to kep recently accessed values in the cache. Addresses that CPU has not accessed (at least for some time) remain in main memory. Data generally appears in cache memory since most memory accesses are to recently accessed variables. Though a program may spend much time executing code in one place, it will eventually call a procedure or wanter off to some section of code outside of cache memory, when CPU has to go to main memory to fetch the data. A cache hit happens when the CPU accesses memory and finds the data in the cache. Usually, the CPU can access data with zero wait states in such a case. A cache miss occurs if the data is not present in the cache, when the CPU has to read the data from main memory, incurring a performance reduction. Taking advantage of reference locality, the CPU copies data into the cache whenever it accesses an address not already present there. This process saves future wait states. Cache memory handles temporal aspects of memory access, but not spatial aspects. Caching memory locations when you access them will not speed up the program if you access consecutive locations in memory. Most caching systems will read several consecutive bytes from memory (sixteen on the 80486) when cache misses occur. Adding more cache memory or building a two-level caching system (like in figure 3.15) can improve performance by reducing cache misses. Why bother with a two-level cache? The secondary cache generally does not operate at zero wait states. The circuitry to support 262,144 bytes (common) of 10 ns memory (20 ns total access time) would be very expensive. So most system designers use slower memory which requires one or two wait states. This is still much faster than main memory. Combined with the on-chip cache, you can get better performance from the system. Consider a system with an 80% hit ratio. If the secondary cache requires two cycles for each memory access and three for the first, then a cache miss on the on-chip cache will require six clock cycles. The average system performance will be two clocks per memory access. The secondary cache can also update its values in parallel with the CPU; thus, the number of cache misses goes down. By writing a program carefully to take advantage of the way the cache memory system works, you can improve your program’s performance. By colocating variables you commonly use together in the same cache line, you can force the cache system to load these variables as a group, saving extra wait states on each access. If you organize your program so that it tends to execute the same sequence of instructions repeatedly, it will have a high degree of temporal locality of reference and will, therefore, execute faster.

Hypothetical processors.
We will use the 886, 8286, 8486, and 8686 processors (extreme simplifications of various processors). They highlight the important features of the 80x86 architecture. They are all identical except for the way they execute instructions. They all have the same register set, and they "execute" the same instruction set.

CPU registers. 
CPU registers are special memory locations constructed from flip-flops not part of main memory. The CPU implements them on-chip. Different members have different register sizes. The 886, 8286, 8486, and 8686 CPUs have four registers all sixteen bits wide. All arithmetic and location operations occur in the registers. Let us give a name to each register:

AX (the accumulator register)
BX (the base address register) 
CX (the count register)
DX (the data register)

x86 processors also have an instruction pointer register which contains the address of the next instruction to execute. There is also a flags register that holds the comparison result. It remembers if one value was less than, equal to, or greater than another value. Registers are much faster than memory. Accessing data in a register usually takes zero clock cycles, whereas accessing memory takes one or more clock cycles. Try to keep variables in the registers. Register sets are very small and most registers have special purposes which limit their use as variables, but they are still an excellent place to store temporary data.

The arithmetic and logical unit.
The arithmetic and logical unit (ALU) is where most of the action takes place inside the CPU. f you want to add the value five to the AX register, the CPU copies the value from AX into the ALU, sends the value five to the ALU, instructs the ALU to add these two values together, and moves the result back into the AX register. 

The bus interface unit.
The BIU is responsible for controlling address and data buses when accessing main memory. If a cache is present on the CPU chip then the BIU is also responsible for accessing data in the cache.

The control unit and instruction sets.
How does a CPU perform assigned chores? This is accomplished by giving the CPU a fixed set of commands, or instructions, to work on. To keep the number of logic gates (used to execute instructions) to a reasonably small set, designers must restrict the number and complexity of the commands the CPU recognizes. This small set of commands is the CPU’s instruction set. Programs in ore-von Neumann systems were hard-wired into the circuitry. The computer's wiring determined what problem was solved. The next advance in computer design was the programmable computer system that allowed a computer programmer to easily rewire the computer system using a sequence of sockets and plug wires. A major difficulty with this scheme is that the number of possible instructions is severely limited by the number of sockets one could physically place on each row. CPU designers discovered that with a small amount of additional logic circuitry, they could reduce the number of sockets required from n holes for n instructions to log2(n) holes for n instructions. They assigned a numeric code to each instruction and encoded that instruction as a binary number (see figure 3.17). This addition requires eight logic functions to decode the A, B, and C bits from the patch panel, but the extra circuitry is well worth the cost because it reduces the number of sockets that must be repeated for each instruction. Many instructions are not stand-alone. move moves data from one location to another, e.g. from one register to another. It therefore requires two operands: a source and destination operand. The designer usually encodes these source and destination operands as part of the machine instruction. Figure 3.17 shows a possible combination of sockets. move would move data from the source register to the destination register, add would add the value of the source register to the destination register, etc. The VNA provides the concept of a stored program. Patch panel programming limits the number of program steps by the number of rows of sockets available. von Neumann and others figured they could store the binary equivalents of a machine program in main memory and fetch each program from memory, load it into a special decoding register that connected directly to the instruction decoding circuitry of the CPU. The trick was to add more circuitry to the CPU. This circuitry, the control unit (CU), fetches instructions codes (operation codes or opcodes) from memory and moves them to the instruction decoding register. The CU contains the instruction pointer that contains the address of an executable instruction. The CU fetches this instruction’s code from memory and places it in the decoding register for execution. After executing the instruction, the CU increments the instruction pointer and fetches the next instruction from memory for execution, and so on. 

The x86 instruction set. 
The x86 CPUs provide 20 basic instruction classes. Seven of these instructions have two operands, eight of these instructions have a single operand, and five instructions have no operands at all. The instructions are mov (two forms), add, sub, cmp, and, or, not, je, jne, jb, jbe, ja, jae, jmp, brk, iret, halt, get, and put. The two forms of mov are shown below.

mov 	reg, reg/memory/constant 
mov 	memory, reg

reg is any of ax, bx, cx, or dx, constant is a numeric constant (using hexadecimal notation), and memory is an operand specifying a memory location. The arithmetic and logical instructions are shown below.

add 	reg, reg/memory/constant 
sub 	reg, reg/memory/constant 
cmp 	reg, reg/memory/constant 
and 	reg, reg/memory/constant 
or 	reg, reg/memory/constant 
not 	reg/memory

The add instruction adds the value of the second operand to the first (register) operand, leaving the sum in the first operand. The sub instruction subtracts the value of the second operand from the first, leaving the difference in the first operand. The cmp instruction compares the first operand against the second and saves the result of this comparison for use with one of the conditional jump instructions. The and and or instructions compute the corresponding bitwise logical operation on the two operands and store the result into the first operand. The not instruction inverts the bits in the single memory or register operand. Control transfer instructions interrupt the sequential execution of instructions in memory and transfer control to some other point in memory either unconditionally, or after testing the result of the previous cmp instruction. They include the following:

ja	dest	-- Jump if above
jae	dest	-- Jump if above or equal
jb	dest	-- Jump if below
jbe	dest	-- Jump if below or equal
je	dest	-- Jump if equal
jne	dest	-- Jump if not equal
jmp	dest	-- Unconditional jump
iret		-- Return from an interrupt

The first six instructions in this class let you check the result of the previous cmp instruction for greater than, greater or equal, less than, less or equal, equality, or inequality. For example, if you compare the ax and bx registers with the cmp instruction and execute the ja instruction, the x86 CPU will jump to the specified destination location if ax was greater than bx. If ax is not greater than bx, control will fall through to the next instruction in the program. The jmp instruction unconditionally transfers control to the instruction at the destination address. The iret instruction returns control from an interrupt service routine. The get and put instructions let you read and write integer values. Get will stop and prompt the user for a hexadecimal value and then store that value into the ax register. put displays (in hexadecimal) the value of the ax register. The remaining instructions do not require any operands, they are halt and brk. halt terminates program execution and brk stops the program in a state that it can be restarted. 

Addressing modes.
x86 instructions use five different operand types: registers, constants, and three memory addressing schemes. Each is an addressing mode. x86 processors support the register addressing mode, the immediate addressing mode, the indirect addressing mode, the indexed addressing mode, and the direct addressing mode. Register operands are easiest to understand. Consider the code below.

mov	ax, bx

It copies the value of bx into ax. bx remains the same. Constants are easy to deal with:

mov	ax, 25

They load their respective registers with the specified hexadecimal constant. There are three addressing modes dealing with accessing data in memory and they take the following forms:

mov	ax, [1000] 
mov	ax, [bx]
mov	ax, [1000+bx]

The first instruction above uses the direct addressing mode to load ax with the 16 bit value stored in memory starting at location 1000 hex. `mov ax, [bx]` loads ax from the memory location specified by the contents of the bx register. This is indirect addressing. Rather than using the value in bx, it accesses to the memory location whose address appears in bx. Note that

mov 	bx, 1000 
mov 	ax, [bx]

is equivalent to

mov	ax, [1000]

which is preferable. Indirection can be faster, shorter, and better in some cases. The last memory addressing mode is indexed addressing. An example is shown below.

mov	ax, [1000+bx]

This instruction adds the contents of bx with 1000 to produce the address of the memory value to fetch. 

Encoding x86 instructions.
A normal CPU opcode uses a certain number of bits to denote the instruction class and a certain number to encode these fields. A basic instruction on an x86 is typically one or three bytes long. It consists of a single byte containing three fields. The HO three bits define the instruction class, providing eight combinations. With twenty instruction classes, we need to pull some tricks. Refer to figure 3.19 to understand this next part. The basic opcode encodes mov (two classes where the rr field or the mmm field specifies the destination), add, sub, cmp, and, and or. The special instruction class allows us to expand the number of available instruction classes. To determine a particular instruction's opcode, you need to select the appropriate bits for the iii, rr, and mmm fields only. To encode `mov ax, bx` you would select iii = 110 (mov reg, reg), rr = 00 (ax), and mmm = 001 (bx). This produces a one byte instruction 11000001 or 0C0h. Some instructions require more than one byte. For instance, `mov ax, [1000]` and `mov ax, [2000]` would produce the same opcode encoding (0C6h). To encode an address for the [xxxx] or [xxxx+bx] addressing modes, or to encode the constant for the immediate addressing mode, follow the opcode with the sixteen-bit address or constant with the LO byte immediately following the opcode in memory and the HO byte after that. The three-byte encoding for `mov ax, [1000]` would be 0C6h, 00h, 10h, and the three byte encoding for `mov ax, [2000]` would be 0C6h, 00h, 20h. special handles several zero and one operand instructions as shown in figures 3.20 and 3.21. 00 expands the instruction set with a set of zero-operand instructions (3.21). 01 provides all seven jump instructions. 10 encodes the not instruction which inverts all bits in the designation register or memory operand. 11 is unassigned and currently halts the processor with an illegal instruction error. jmp copies the sixteen-bit immediate value (address) following the opcode into the IP register. Effectively, the program jumps from the point of the jmp instruction to the instruction at the target address. jmp is unconditional as it always transfers control to the target address. The other six test some condition and jump if it is met. These instructions are executed immediately after a cmp instruction. Note that there are eight possible jump opcodes, but the x86 uses only seven of them. The eighth opcode is another illegal opcode. The zero operand instructions appear in 3.21. The first three are illegal opcodes. `brk` pauses the CPU until the user manually restarts it. `brk` is primarily used to observe results. `iret` (interrupt return) returns control from an interrupt service routine. `halt` terminates program execution. `get` reads a hex value from the user and returns it in the `ax` register. `put` outputs the value in the `ax` register.

Step-by-step instruction execution.
x86 CPUs do not complete execution of an instruction in one clock cycle; they execute several steps for one instruction. For example, the CU issues the following commands to execute `mov reg, reg/memory/constant`:

- Fetch the instruction byte from memory.
- Update the `ip` register to point to the next byte.
- Decode the instruction.
- Fetch a sixteen-bit instruction operand from memory (if required).
- Update `ip` to point beyond the operand (if required).
- Compute the address of the operand of the operand (if required).
- Fetch the operand.
- Store the fetched value into the destination register.

Page 107 and part of page 108 detail this process. For `mov memory, reg`, the CPU does the following:

- Fetch the instruction byte from memory (one clock cycle).
- Update `ip` to point to the next byte (one clock cycle).
- Decode the instruction (one clock cycle).
- Fetch an operand from memory (if required) (zero cycles if `[bx]` addressing mode, one cycle if `[xxxx]`, `[xxxx+bx]`, or `xxxx` addressing mode and the value `xxxx` immediately following the opcode starts on an even address, or two clock cycles if the value `xxxx` starts at an odd address).
- Update `ip` to point beyond the operand (if required) (zero cycles if no such operand, one clock cycle if the operand is present).
- Compute the address of the operand (zero cycles if the addressing mode is not `[bx]` or `[xxxx+bx]`, one cycle if the addressing mode is `[bx]`, or two cycles if the addressing mode is `[xxxx+bx]`).
- Get the value of the register to store (one clock cycle).
- Store the fetched value into the destination location (one cycle if a register, two cycles if a word-aligned memory operand, or three clock cycles if an odd-address aligned memory operand).

`add`, `sub`, `cmp`, `and`, and `or` do the following:

- Fetch the instruction byte from memory (one clock cycle).
- Update `ip` to point to the next byte (one clock cycle).
- Decode the instruction (one clock cycle).
- Fetch a constant operand from memory (if required) (zero cycles if `[bx]` addressing mode, one cycle if `[xxxx]`, `[xxxx+bx]`, or `xxxx` addressing mode and the value `xxxx` immediately following the opcode starts on an even address, or two clock cycles if the value `xxxx` starts at an odd address).
- Update `ip` to point beyond the constant operand (if required) (zero or one cycle).
- Compute the address of the operand (zero cycles if the addressing mode is not `[bx]` or `[xxxx+bx]`, one cycle if the addressing mode is `[bx]`, or two cycles if the addressing mode is `[xxxx+bx]`).
- Get the value of the operand and send it to the ALU (zero cycles if a constant, one cycle if a register, two cycles if a word-aligned memory operand, or three clock cycles if an odd-address aligned memory operand).
- Fetch the value of the first operand (a register) and send it to the ALU (one cycle).
- Instruct the ALU to perform the operation on the values (one cycle).
- Store the result into the first register operand (one cycle).

`not` takes a similar form to the above. Conditional jump instructions do the following:

- Fetch the instruction byte from memory (one cycle).
- Update `ip` to point to the next byte (one cycle).
- Decode the instructions (one cycle).
- Fetch the target address operand from memory (one cycle if `xxxx` is at an even address, two cycles if at an odd address).
- Update `ip` to point beyond the address (one cycle).
- Test the "less than" and "equality" CPU flags (one cycle).
- If the flag values are appropriate for the particular conditional jump, the CPU copies the sixteen-bit constant into the `ip` register (zero cycles if no branch, one cycle if branch occurs).

Differences between processors.
Each processor has differences in prefetch queues, caches, pipelines, and superscalar designs. These inconsistencies serve a purpose. Pages 109 to 124 highlight these differences. 

I/O.
The three forms of I/O are I/O-mapped I/O, memory-mapped I/O, and direct memory access (DMA). I/O-mapped I/O uses special instructions to transfer data between the system and the outside world. Memory-mapped I/O uses special memory locations in the address space of the CPU to communicate with external devices. DMA is memory-mapped I/O where the peripheral device reads and writes memory without going through the CPU. To send data to an output device, the CPU moves that data to a special memory location or to an address in the memory address space. The CPU moves data from the address of an input device into the CPU to read data from that device. I/O looks very similar to a memory read or write operation other than more wait states associated with a peripheral device. An I/O port looks like a memory cell to the computer but contains connections outside the computer. It typically uses a latch rather than a flip-flop to implement the cell. When CPU writes to the latch address, the latch device captures the data and makes it available on a set of wires external to the CPU and memory system (figure 3.32). I/O ports can be read-only, write-only, or read/write. Figure 3.32 shows a write-only port and 3.33 shows a read/write port. The differences to us between I/O-mapped and memory mapped I/O operations are instructions. For memory-mapped I/O, any instruction that accesses memory can access a memory-mapped I/O port. `mov`, `add`, `sub`, `cmp`, `and`, `or`, and `not` can read memory. `mov` and `not` can write to memory. I/O-mapped I/O uses special instructions to access I/O ports, like `get` or `put`. Both of these subsystems require the CPU to move data between the peripheral device and main memory. For example, to input a sequence of ten bytes from an input port and store these bytes into memory the CPU must read each value and store it into memory. For high-speed I/O devices the CPU may be too slow when processing this data a byte at a time. Such devices generally contain an interface to the CPU’s bus so it directly read and write memory. This is known as direct memory access since the peripheral device accesses memory directly without using the CPU as an intermediary, allowing the I/O operation to run in parallel with other CPU operations. Concurrent processing only happens if the CPU has a cache and is using the cache such that the bus is free. Even if the CPU must wait for the DMA operation to complete, I/O is still much faster. 

Interrupts and polled I/O.
Some I/O devices cannot accept data at an arbitrary rate (a printer). The CPU needs a mechanism to coordinate data transfer between the system and peripheral devices. One way is providing status bits in a secondary input port. For example, a one in a bit in an I/O port can tell the CPU that a printer is ready to accept more data. Assume that the printer data port is memory-mapped to the address 0FFE0h and the printer status port is bit zero of the memory-mapped port 0FFE2h. The following code waits until the printer is ready to accept a byte of data and then writes the byte in the LO byte of `ax` to the printer port.

0000: mov	bx, [FFE2]
0003: and	bx, 1
0006: cmp	bx, 0
0009: je		0000
000C: mov	[FFE0], ax

The first instruction fetches the data at the status input port. The second instruction logically ANDs this value with one to clear bits one through fifteen and set bit zero to the current status of the printer port. Note that this produces the value zero in `bx` if the printer is busy, it produces the value one in `bx` if the printer is ready to accept additional data. The third instruction checks `bx` to see if it contains zero (the printer is busy). If the printer is busy, this program jumps back to location zero and repeats this process over and over again until the printer status bit is one. Let's look at this key press–reading code that presumes that the keyboard status bit is bit zero of address 0FFE6h and the ASCII code of the key appears at address 0FFE4h when bit zero of location 0FFE6h contains a one.

0000: mov	bx, [FFE6]
0003: and	bx, 1
0006: cmp	bx, 0
0009: je		0000
000C: mov	ax, [FFE4]

Polling is when the CPU constantly tests a port to see if data is available there. Polling is inherently inefficient. Polling is how early personal computer systems like the Apple II read data from the keyboard. Another problem with polling is that if too much time passes between checking the keyboard status port, the user could press a second key and the first keystroke would be lost. The solution here is providing an interrupt mechanism. An interrupt is an external hardware event that causes the CPU to interrupt the current instruction sequence and call an interrupt service routine (ISR). An ISR saves all the registers and flags, does whatever operation is necessary to handle the source of the interrupt, restores the registers and flags, and then resumes execution of the code it interrupted. The ISR quickly processes the request in the background, allowing some other computation to proceed normally in the foreground. CPUs supporting interrupts must have a mechanism allowing the programmer to specify the address of the ISR to execute when an interrupt occurs. An interrupt vector is a memory location containing the address of the ISR to execute when an interrupt occurs. After an ISR completes its operation, it generally returns control to the foreground with a return from interrupt instruction: the `iret` instruction. A typical interrupt-driven input system uses the ISR to read data from an input port and buffer it up whenever data becomes available. The foreground program can read that data from the buffer at its leisure without losing any data from the port. Likewise, a typical interrupt-driven output system (that gets an interrupt whenever the output device is ready to accept more data) can remove data from a buffer whenever the peripheral device is ready to accept new data.